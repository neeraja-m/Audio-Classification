{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import torchvision as tv\n","from torch.utils.data import DataLoader,random_split,Dataset,TensorDataset\n","import torch.nn as nn\n","from torchvision import transforms,datasets\n","import torch.optim as optim\n","from torchmetrics import ConfusionMatrix\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","from torchmetrics.classification import MulticlassAccuracy\n","from torchvision.transforms import Resize,ToTensor,Compose\n","import librosa, IPython\n","import librosa.display as lplt\n","import random\n","from tqdm import tqdm\n","import torchaudio\n","from torchaudio.transforms import Resample\n","import json\n","import math\n","from sklearn.metrics import confusion_matrix\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Audio Classification using LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original'\n","genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n","\n","# define dataset split\n","training_size = 0.7\n","validation_size = 0.2\n","testing_size = 0.1\n","batch_size = 64\n","\n","# define epochs\n","epochs = 50\n","\n","# define seed for reproducibility \n","seed=7\n","random.seed(seed)\n","torch.manual_seed(seed)\n","device = \"cuda\"\n","\n","# get sample rate and audio length\n","audio_example = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/country/country.00000.wav'\n","audio_data, sr = librosa.load(audio_example)\n","sample_num = len(audio_data)\n","\n","aud_length = sample_num / sr\n","print(\"Sample rate:\",sr)\n","print(\"Audio length:\",aud_length)\n","print(\"Sequence length:\",sample_num)\n","\n","# extract MFCCs with coefficient of 13\n","mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","music_array = [] \n","genres = [] \n","for root, dirs, files in os.walk(data_path):\n","    for name in files:\n","        filename = os.path.join(root, name)\n","        # skip corrupt file for processing\n","        if filename != '/data/genres_original/jazz/jazz.00054.wav':\n","            music_array.append(filename)\n","            genres.append(filename.split(\"/\")[5])\n","            \n","        "]},{"cell_type":"markdown","metadata":{},"source":["### Feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define file path for extarcted feature dictionary\n","fe_file=\"/kaggle/working/fe.json\"\n","samples = aud_length*sr\n","\n","# function for feature extraction\n","def feature_extraction(data_path, fe_file, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n","    \n","     # dictionary to store mfccs, target labels, and corresponding mappings\n","    audio_info = {\n","        \"mapping\": [],\n","        \"labels\": [],\n","        \"mfcc\": []\n","    }\n","    \n","    samples_per_seg = int(samples / num_segments)\n","    mfcc_vectors_per_seg = math.ceil(samples_per_seg / hop_length)\n","\n","    \n","    # loop through all genre sub-folder\n","    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(data_path)):\n","\n","        if dirpath is not data_path:\n","            \n","            # append genre label\n","            genre_label = dirpath.split(\"/\")[-1]\n","            audio_info[\"mapping\"].append(genre_label)\n","\n","            # process all audio files in genre sub-dir\n","            for f in filenames:\n","\n","            # load audio file\n","\n","                file_path = os.path.join(dirpath, f)\n","                \n","            # skip corrupt file jazz.00054\n","                if (file_path != '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav') and (file_path !='/kaggle/working/final/jazz/jazz.00054.wav'):\n","\n","                    signal, sample_rate = librosa.load(file_path, sr=sr)\n","                \n","                \n","                    # process audio segments (split into 5) of audio file\n","                    for s in range(num_segments):\n","\n","                        # find start and finish sample of current audio segment\n","                        start = samples_per_seg * s\n","                        finish = start + samples_per_seg\n","\n","                        # extract mfccs and transpose\n","                        mfcc = librosa.feature.mfcc(y = signal[start:finish], sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n","                        mfcc = mfcc.T\n","\n","                        # if mfcc feature length has expected number of vectors, append to dictionary\n","                        if len(mfcc) == mfcc_vectors_per_seg:\n","                            audio_info[\"mfcc\"].append(mfcc.tolist())\n","                            audio_info[\"labels\"].append(i-1)\n","\n","    # save extarcted mfccs to defined json file\n","    with open(fe_file, \"w\") as fp:\n","        json.dump(audio_info, fp, indent=4)\n","        \n","#     torch.save(data,fe_file)\n","\n","feature_extraction(data_path, fe_file, num_segments=5)\n"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# json_path = \"/kaggle/working/fe.json\"\n","\n","\n","def load_fe_data(json_path):\n","   \n","    with open(json_path, \"r\") as fp:\n","        fe_data = json.load(fp)\n","\n","    X = np.array(fe_data[\"mfcc\"])\n","    y = np.array(fe_data[\"labels\"])\n","    z = np.array(fe_data['mapping'])\n","    return X, y, z\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X,y,z = load_fe_data('/kaggle/working/fe.json')\n","\n","sample_num = len(X)\n","\n","train_samples = int(training_size * sample_num)\n","val_samples = int(validation_size * sample_num)\n","test_samples = sample_num - train_samples - val_samples\n","\n","# convert numpy arrays to tensors for dataset\n","ds = TensorDataset(torch.from_numpy(X).unsqueeze(1), torch.from_numpy(y))\n","\n","# split data as defined\n","train_data, val_data, test_data = random_split(ds, [train_samples, val_samples, test_samples])\n","\n","# load data using DataLoader with batch size 64\n","train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n","\n","# shape of mfccs\n","print(\"MFCCs shape:\", mfccs.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### RNN with LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class RNN(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size, num_layers,seq_length):\n","        super(RNN, self).__init__()\n","        # define network layers\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,dropout=0.8)\n","        self.fc = nn.Linear(216 * hidden_size, 10)\n","        self.dropout= nn.Dropout(0.8)\n","        \n","    def forward(self, x):\n","        \n","        # set the initial states of hidden and cell states and move to gpu\n","        hidden_1 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","        cell_1 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n","\n","        # forward pass through LSTM\n","        output, _ = self.lstm(x, (hidden_1, cell_1))\n","        \n","        # flatten output to feed to linear layer\n","        output = output.reshape(output.shape[0], -1)\n","        \n","        # out = self.dropout(out)\n","        \n","        #final output through linear layer\n","        output = self.fc(output)\n","        return output    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define parameters\n","input_size =  mfccs.shape[0]\n","hidden_size =256\n","num_layers=3\n","seq_length= 663300\n","\n","# instantiate RNN model\n","model = RNN(input_size, hidden_size, num_layers,seq_length).to(device)\n","\n","# define loss function \n","loss_fn = nn.CrossEntropyLoss()\n","\n","# define optimiser with learning rate 0.0001 and weight decay for regularisation\n","optimizer = optim.Adam(model.parameters(), lr=0.0001,weight_decay=0.1)\n","\n","# training loop\n","for epoch in range(epochs):\n","    for batch_idx, (data, targets) in enumerate( tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n","        \n","        if data is None:\n","            continue \n","            \n","        data = data.to(device=device).squeeze(1)\n","        targets = targets.to(device=device)\n","        data = data.float()\n","\n","        # forward pass\n","        scores = model(data)\n","        \n","        # get loss value\n","        loss = loss_fn(scores, targets)\n","\n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        \n","        optimizer.step()\n","\n","        \n","# define confusion matrix for 10 class output\n","conmat=ConfusionMatrix(task='multiclass',num_classes=10).cuda()\n","\n","# calculate accuracy on given model\n","def get_accuracy(loader, model):\n","    correct = 0\n","    num_samples = 0\n","\n","    # set to evaluate model\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device=device).squeeze(1)\n","            y = y.to(device=device)\n","            x=x.float()\n","            scores = model(x)\n","            _, predictions = scores.max(1)\n","            correct += (predictions == y).sum()\n","            num_samples += predictions.size(0)\n","            \n","            #update confusion matrix values\n","            conmat.update(scores,y)\n","\n","    # Toggle model back to train\n","    model.train()\n","    return correct / num_samples\n","\n","# output accuracy\n","print(f\"Training set accuracy: {get_accuracy(train_loader, model)*100:2f}\")\n","print(f\"Testing set accuracy: {get_accuracy(test_loader, model)*100:.2f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# produce confusion matrix heatmap\n","x=conmat.compute().cpu().numpy()\n","\n","# normalise raw values to convert to percentages\n","conmat_normed = x.astype('float') / x.sum(axis=1)[:, np.newaxis]\n","conmat_per = np.round(conmat_normed * 100, 2)\n","\n","# plot confusion matrix heatmap\n","plt.figure(figsize=(10, 8))\n","sb.heatmap(conmat_per, annot=True, cmap='rocket', fmt='g')\n","plt.ylabel('True Label')\n","plt.xlabel('Predicted Label')\n","plt.show()\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":568973,"sourceId":1032238,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
