{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import torchvision as tv\n","from torch.utils.data import DataLoader,random_split,Dataset,TensorDataset\n","import torch.nn as nn\n","from torchvision import transforms,datasets\n","import torch.optim as optim\n","from torchmetrics import ConfusionMatrix\n","import matplotlib.pyplot as plt\n","import seaborn as sb\n","from torchmetrics.classification import MulticlassAccuracy\n","from torchvision.transforms import Resize,ToTensor,Compose\n","import librosa, IPython\n","import librosa.display as lplt\n","import random\n","from tqdm import tqdm\n","from torchaudio.transforms import Resample\n","import json\n","import math\n","from sklearn.metrics import confusion_matrix\n","import soundfile as sf\n","import shutil\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Audio Generation using GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["data_path = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original'\n","genres = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n","\n","# define dataset split\n","training_size = 0.7\n","validation_size = 0.2\n","testing_size = 0.1\n","batch_size = 64\n","\n","# define epochs\n","epochs = 50\n","\n","# define seed for reproducibility \n","seed=7\n","random.seed(seed)\n","torch.manual_seed(seed)\n","device = \"cuda\"\n","\n","# get sample rate and audio length\n","audio_example = '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/country/country.00000.wav'\n","audio_data, sr = librosa.load(audio_example)\n","sample_num = len(audio_data)\n","\n","aud_length = sample_num / sr\n","print(\"Sample rate:\",sr)\n","print(\"Audio length:\",aud_length)\n","print(\"Sequence length:\",sample_num)\n","\n","# extract MFCCs with coefficient of 13\n","mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","music_array = [] \n","genres = [] \n","for root, dirs, files in os.walk(data_path):\n","    for name in files:\n","        filename = os.path.join(root, name)\n","        # skip corrupt file for processing\n","        if filename != '/data/genres_original/jazz/jazz.00054.wav':\n","            music_array.append(filename)\n","            genres.append(filename.split(\"/\")[5])\n","            \n","        "]},{"cell_type":"markdown","metadata":{},"source":["### Feature Extraction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define file path for extarcted feature dictionary\n","fe_file=\"/kaggle/working/fe.json\"\n","samples = aud_length*sr\n","\n","# function for feature extraction\n","def feature_extraction(data_path, fe_file, num_mfcc=13, n_fft=2048, hop_length=512, num_segments=5):\n","    \n","     # dictionary to store mfccs, target labels, and corresponding mappings\n","    audio_info = {\n","        \"mapping\": [],\n","        \"labels\": [],\n","        \"mfcc\": []\n","    }\n","    \n","    samples_per_seg = int(samples / num_segments)\n","    mfcc_vectors_per_seg = math.ceil(samples_per_seg / hop_length)\n","\n","    \n","    # loop through all genre sub-folder\n","    for i, (dirpath, dirnames, filenames) in enumerate(os.walk(data_path)):\n","\n","        if dirpath is not data_path:\n","            \n","            # append genre label\n","            genre_label = dirpath.split(\"/\")[-1]\n","            audio_info[\"mapping\"].append(genre_label)\n","\n","            # process all audio files in genre sub-dir\n","            for f in filenames:\n","\n","            # load audio file\n","\n","                file_path = os.path.join(dirpath, f)\n","                \n","            # skip corrupt file jazz.00054\n","                if (file_path != '/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav') and (file_path !='/kaggle/working/final/jazz/jazz.00054.wav'):\n","\n","                    signal, sample_rate = librosa.load(file_path, sr=sr)\n","                \n","                \n","                    # process audio segments (split into 5) of audio file\n","                    for s in range(num_segments):\n","\n","                        # find start and finish sample of current audio segment\n","                        start = samples_per_seg * s\n","                        finish = start + samples_per_seg\n","\n","                        # extract mfccs and transpose\n","                        mfcc = librosa.feature.mfcc(y = signal[start:finish], sr=sample_rate, n_mfcc=num_mfcc, n_fft=n_fft, hop_length=hop_length)\n","                        mfcc = mfcc.T\n","\n","                        # if mfcc feature length has expected number of vectors, append to dictionary\n","                        if len(mfcc) == mfcc_vectors_per_seg:\n","                            audio_info[\"mfcc\"].append(mfcc.tolist())\n","                            audio_info[\"labels\"].append(i-1)\n","\n","    # save extarcted mfccs to defined json file\n","    with open(fe_file, \"w\") as fp:\n","        json.dump(audio_info, fp, indent=4)\n","        \n","#     torch.save(data,fe_file)\n","\n","feature_extraction(data_path, fe_file, num_segments=5)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# json_path = \"/kaggle/working/fe.json\"\n","\n","\n","def load_fe_data(json_path):\n","   \n","    with open(json_path, \"r\") as fp:\n","        fe_data = json.load(fp)\n","\n","    X = np.array(fe_data[\"mfcc\"])\n","    y = np.array(fe_data[\"labels\"])\n","    z = np.array(fe_data['mapping'])\n","    return X, y, z\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["X,y,z = load_fe_data('/kaggle/working/fe.json')\n","\n","sample_num = len(X)\n","\n","train_samples = int(training_size * sample_num)\n","val_samples = int(validation_size * sample_num)\n","test_samples = sample_num - train_samples - val_samples\n","\n","# split data as defined\n","train_data, val_data, test_data = random_split(ds, [train_samples, val_samples, test_samples])\n","\n","# load data using DataLoader with batch size 64\n","train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n","val_loader = DataLoader(val_data, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n","\n","# shape of mfccs\n","print(\"MFCCs shape:\", mfccs.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### GAN"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# define generator\n","class Generator(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(Generator, self).__init__()\n","        \n","        self.model = nn.Sequential(\n","                # feed initial ConvTranspose1d layer latent dimension of 1000\n","            \n","                nn.ConvTranspose1d(input_dim, 1024, 3, 15, 0, bias=False),\n","                nn.ReLU(0.3),\n","                nn.ConvTranspose1d(1024, 512, 3, 15, 2, bias=False),\n","                nn.ReLU(0.3),\n","                nn.ConvTranspose1d(512, 256, 3, 15, 2, bias=False),\n","                nn.ReLU(0.3),\n","                nn.ConvTranspose1d(256, 216, 3, 10, 2, bias=False),\n","                nn.ReLU(0.3),\n","                nn.ConvTranspose1d(216, 216, 3, 10, 2, bias=False),\n","                nn.ReLU(0.3),\n","        )\n","\n","        \n","    def forward(self, x):\n","        \n","        # permute shape to fit network input \n","        \n","        x = x.unsqueeze(-1)\n","        x = torch.permute(x,(0,1,2))\n","    \n","\n","        return self.model(x)\n","    \n","# define discriminator \n","class Discriminator(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Discriminator, self).__init__()\n","\n","        # input shape of discriminator will be output shape of generator\n","        \n","        self.model = nn.Sequential(\n","            nn.Conv1d(input_dim, 128, 3, 4, 2, bias=False),\n","            nn.ReLU(0.2),\n","            nn.Conv1d(128, 256, 3, 4, 2, bias=False),\n","            nn.ReLU(0.2),\n","            nn.Conv1d(256, 512, 3, 4, 2, bias=False),\n","            nn.ReLU(0.2),\n","            nn.Conv1d(512, 3998, 3, 4, 2, bias=False),\n","            nn.ReLU(0.2))\n","        \n","    def forward(self, x):\n","#         x = torch.permute(x,(0,2,1))\n","\n","        return self.model(x)\n","\n","# define latent dimension\n","input_dim= 1000\n","\n","# instantiate generator and discriminator\n","generator = Generator(input_dim,13).to(device)\n","discriminator = Discriminator(216).to(device)\n","\n","\n","# define loss function \n","criterion = nn.BCEWithLogitsLoss()\n","\n","# define optimisers\n","generator_optimizer = optim.Adam(generator.parameters(), lr=0.0001)\n","discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0001)\n","\n","\n","# training loop\n","num_epochs = 50\n","for epoch in range(num_epochs):\n","    for(data, targets) in (train_loader):\n","  \n","        data = data.to(device=device).squeeze(1)\n","        data = data.float()\n","        \n","        # generate noise to feed to generator\n","        noise = torch.randn(batch_size, input_dim).to(device=device)\n","    \n","        # generate fake samples from generator \n","        fake_music = generator(noise)\n","\n","        # train the discriminator network\n","        discriminator_optimizer.zero_grad()\n","        \n","        # get output from discriminator fed real music\n","        real_output = discriminator(data)\n","        \n","        # get output from discriminator fed fake music \n","        fake_output = discriminator(fake_music.detach())  \n","        \n","        # get discriminator loss\n","        d_loss_real = criterion(real_output, torch.ones_like(real_output))\n","        d_loss_fake = criterion(fake_output, torch.zeros_like(fake_output))\n","        d_loss = d_loss_real + d_loss_fake\n","        d_loss.backward()\n","        discriminator_optimizer.step()\n","\n","        # train generator\n","        generator_optimizer.zero_grad()\n","        \n","        # get fake output for discriminator\n","        fake_output = discriminator(fake_music.detach())\n","        \n","        # get generator loss\n","        g_loss = criterion(fake_output, torch.ones_like(fake_output))  \n","        g_loss.backward()\n","        generator_optimizer.step()\n","\n","    print(f'Epoch [{epoch+1}/{num_epochs}], 'f'Disc_loss: {d_loss.item():.4f}, Gen_loss: {g_loss.item():.4f}')\n"]},{"cell_type":"markdown","metadata":{},"source":["### Save generated audio"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# list to store audio samples\n","generated_audio_samples = []\n","\n","# loop to generate 64 samples\n","for _ in range(1):\n","    with torch.no_grad():\n","        \n","        # generate noise to feed generator\n","        s_noise = torch.randn(batch_size, 1000, device=device)  \n","        gen_audio = generator(s_noise).detach().cpu()    \n","\n","        # processing samples to save in right format\n","        for i in range(gen_audio.shape[0]):\n","            \n","            # average channels\n","            mono_fin_audio = gen_audio[i].mean(0)\n","    \n","            # normalise audio and convert to numpy array\n","            mono_fin_audio = torch.clamp(mono_fin_audio, -1, 1).numpy()  \n","            \n","            # save audio file \n","            filename = f'/kaggle/working/generated_audio_{i}.wav'\n","            sf.write(filename, mono_fin_audio, 22050) \n","  "]},{"cell_type":"markdown","metadata":{},"source":["### Re-process generated audio"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# define source and destination files\n","\n","source = \"/kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original\"\n","des = \"/kaggle/working/final\"\n","\n","# duplicate original genre folders\n","shutil.copytree(source, des)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","generated_ds_path = '/kaggle/working/'\n","\n","ds_path = '/kaggle/working/final'\n","to_move = [file for file in os.listdir(generated_ds_path) if file.endswith(\".wav\")]\n","\n","# move generated audio files to respective genre folders if they are WAV files\n","\n","for file in to_move:\n","    source_file = os.path.join(generated_ds_path, file)\n","    destination_file = os.path.join(ds_path, file)\n","    \n","    shutil.move(source_file, destination_file)  "]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":568973,"sourceId":1032238,"sourceType":"datasetVersion"}],"dockerImageVersionId":30673,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
